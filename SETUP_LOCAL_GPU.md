# Lokales Setup mit NVIDIA GPU

**Transcription Provider:** WhisperX (lokal mit GPU-Beschleunigung)  
_Hinweis: Diese Anleitung gilt nur, wenn `TRANSCRIPTION_PROVIDER=whisperx` in .env.local gesetzt ist._

## Dein System
- ‚úÖ 64GB RAM - Perfekt f√ºr gro√üe Modelle
- ‚úÖ 12GB NVIDIA GPU (Pascal) - Ausreichend f√ºr large-v2
- ‚úÖ Empfehlung: **large-v2** oder **large-v3**

## Performance-Erwartung

Mit deiner 12GB Pascal GPU (z.B. GTX 1080 Ti):

| Modell | VRAM | Geschwindigkeit | Qualit√§t |
|--------|------|-----------------|----------|
| large-v2 | ~10GB | 2-4x Echtzeit | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Exzellent |
| large-v3 | ~10GB | 2-4x Echtzeit | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Noch besser |
| medium | ~5GB | 1-2x Echtzeit | ‚≠ê‚≠ê‚≠ê‚≠ê Sehr gut |

**Beispiel:** 5 Minuten Audio = ~10-20 Minuten Transkription

## Schnellstart

### 1. NVIDIA Container Toolkit installieren

```bash
# Docker NVIDIA Support
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg
curl -s -L https://nvidia.github.io/libnvidia-container/$distribution/libnvidia-container.list | \
    sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \
    sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list

sudo apt-get update
sudo apt-get install -y nvidia-container-toolkit
sudo systemctl restart docker
```

### 2. GPU-Test

```bash
# Pr√ºfe ob Docker GPU erkennt
docker run --rm --gpus all nvidia/cuda:12.0.0-base-ubuntu22.04 nvidia-smi
```

Du solltest deine GPU sehen!

### 3. WhisperX mit GPU starten

```bash
# GPU ist bereits in docker-compose.yml aktiviert
docker-compose up -d

# Logs beobachten (erster Start dauert ~5 Minuten - l√§dt Modell)
docker-compose logs -f whisper
```

### 4. Testen

```bash
# Service-Status
curl http://localhost:5000/health

# Sollte zeigen:
# {"status":"healthy","device":"cuda","model":"large-v2","language":"de"}
```

### 5. App nutzen

```bash
# Next.js starten (in neuem Terminal)
npm install
npm run dev
```

App l√§uft auf http://localhost:3000

## Modell wechseln

In `docker-compose.yml`:

```yaml
environment:
  - WHISPER_MODEL=large-v3  # oder large-v2, medium
```

Dann neu starten:
```bash
docker-compose down
docker-compose up -d
```

## Troubleshooting

### GPU wird nicht erkannt

```bash
# Pr√ºfe NVIDIA Driver
nvidia-smi

# Pr√ºfe Docker GPU-Support
docker run --rm --gpus all nvidia/cuda:12.0.0-base-ubuntu22.04 nvidia-smi

# Logs checken
docker-compose logs whisper
```

### Out of Memory

Passiert normalerweise nicht mit 12GB, aber falls doch:

```yaml
environment:
  - WHISPER_MODEL=medium  # Kleineres Modell
```

### Langsam

```bash
# GPU-Auslastung checken (in separatem Terminal)
watch -n 1 nvidia-smi

# Sollte 80-100% GPU-Nutzung zeigen w√§hrend Transkription
```

Falls GPU-Nutzung niedrig:
- Batch-Size in `whisper-service/app.py` erh√∂hen (Zeile 61: `batch_size=16` ‚Üí `batch_size=32`)

## Performance-Optimierung

### Option 1: Batch-Size erh√∂hen

In `whisper-service/app.py`:

```python
result = model.transcribe(audio, batch_size=32, language=language)  # statt 16
```

### Option 2: Flash Attention (f√ºr neuere GPUs)

Falls du eine neuere GPU h√§ttest (Ampere/Ada), k√∂nnte man Flash Attention aktivieren.
Pascal unterst√ºtzt das nicht, aber die Performance ist auch so gut.

### Option 3: Quantisierung

Bereits aktiviert mit `float16` f√ºr GPU (siehe `app.py` Zeile 18).

## Kosten

- ‚ö° **Strom:** ~200-300W w√§hrend Transkription
- üí∞ **API-Kosten:** ‚Ç¨0 (alles lokal!)
- üîí **Datenschutz:** 100% lokal, keine Cloud

## Vergleich zu Cloud

| L√∂sung | Kosten | Geschwindigkeit | Qualit√§t | Datenschutz |
|--------|--------|----------------|----------|-------------|
| **Dein Setup** | Strom | 2-4x Echtzeit | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚úÖ 100% |
| ElevenLabs API | ~$0.10/min | ~1x Echtzeit | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚ùå Cloud |
| OpenAI Whisper API | ~$0.006/min | ~1x Echtzeit | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚ùå Cloud |

**Bei 100 Stunden/Monat:**
- Dein Setup: ~‚Ç¨10 Strom
- ElevenLabs: ~‚Ç¨600
- OpenAI: ~‚Ç¨36

## Empfehlung f√ºr Produktion

Mit deiner Hardware:

‚úÖ **large-v2** - Beste Balance aus Geschwindigkeit und Qualit√§t  
‚úÖ **Alignment aktiviert** - Pr√§zise Zeitstempel  
‚úÖ **Batch-Size 16-32** - Nutzt deine GPU voll aus  
‚úÖ **Deutsch-Modell** - Bereits konfiguriert  

Das Setup ist perfekt f√ºr medizinische Transkription! üéâ

## Wartung

```bash
# Container neu starten
docker-compose restart whisper

# Modell-Cache leeren (bei Problemen)
docker volume rm schreibdienst_whisper-models
docker-compose up -d

# Logs live anschauen
docker-compose logs -f whisper

# Container stoppen
docker-compose down
```

## N√§chste Schritte

1. [X] NVIDIA Container Toolkit installieren
2. [ ] `docker-compose up -d` starten
3. [ ] Erste Transkription testen
4. [ ] Optional: Batch-Size optimieren
5. [ ] In Produktion nehmen!

Bei Fragen: Siehe [README.md](README.md) oder [WHISPERX.md](WHISPERX.md)
